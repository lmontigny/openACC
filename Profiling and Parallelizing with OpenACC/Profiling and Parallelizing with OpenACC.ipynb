{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling and Parallelizing with OpenACC\n",
    "\n",
    "Lab written by Jeff Larkin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following timer counts down to a five minute warning before the lab instance shuts down.  You should get a pop up at the five minute warning reminding you to save your work!  If you are about to run out of time, please see the [Post-Lab](#Post-Lab-Summary) section for saving this lab to view offline later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"timer\" src=\"timer/timer.html\" width=\"100%\" height=\"120px\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Before we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell.  If not, please consult the [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer should be three: 3\n"
     ]
    }
   ],
   "source": [
    "print \"The answer should be three: \" + str(1+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  8 20:19:11 2017       \r\n",
      "+------------------------------------------------------+                       \r\n",
      "| NVIDIA-SMI 352.68     Driver Version: 352.68         |                       \r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GRID K520           On   | 0000:00:03.0     Off |                  N/A |\r\n",
      "| N/A   27C    P8    18W / 125W |     11MiB /  4095MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Your Lab Instance\n",
    "\n",
    "You are required to connect to the lab instance over SSH - much like you would when working on a real system.  You can do this by:\n",
    "* Using a local SSH client, connect to **ec2-54-92-248-21.compute-1.amazonaws.com** with username **ubuntu** and password **FcN7Dy6BtVs**\n",
    "* If you don't have an SSH client installed, you can use the provided <a href=\"ssh\" target=\"_blank\">browser-based client.</a> and the same username and password as above.  \n",
    "  * **NOTE**: If you right-click in the browser-based client, you can select \"Paste from browser\" to easily copy & paste in the password.\n",
    "\n",
    "Once connected, please proceed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab you will profile the provided application using either NVIDIA\n",
    "nvprof or gprof and the PGI compiler. After profiling the application, you will\n",
    "use OpenACC to express the parallelism in the 3 most time-consuming routines.\n",
    "You will use CUDA Unified Memory and the PGI \"managed\" option to manage host\n",
    "and device memories for you. You may use either the `kernels` or `parallel loop` \n",
    "directives to express the parallelism in the code. Versions of the code\n",
    "have been provided in C99 (directory `/home/ubuntu/c99`) and Fortran 90 (directory `/home/ubuntu/f90`). \n",
    "The `nano`, `vim`, and `emacs` file editors are all available to use during this lab. \n",
    "If you are not experienced with Linux text editors, `nano` is the simplest choice.\n",
    "\n",
    "\n",
    "![Lecture 2 steps: Identify and Express Parallelism](files/Lecture-2-Steps.png)\n",
    "\n",
    "As discussed in the associated lecture, this lab will focus solely on *Identifying Parallelism* in the \n",
    "code by profiling the application and *Expressing Parallelism* using OpenACC. We will use CUDA Unified Memory \n",
    "to allow the data used on the GPU to be automatically migrated to and from the GPU as needed. Please be\n",
    "aware that you may see an application slowdown until you have completed each step of this lab. This is expected\n",
    "behavior due to the need to migrate data between the CPU and GPU memories.\n",
    "\n",
    "**Important** You should repeat steps 2 and 3 for each function identified in step 1\n",
    "in order of function importance. Gather a new GPU profile each time and observe\n",
    "how the profile changes after each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Building the code\n",
    "\n",
    "Makefiles have been provided for building both the C and Fortran versions of the code. Change directory to your language of choice and run the `make` command to build the code.\n",
    "\n",
    "### C/C++\n",
    "\n",
    "```\n",
    "$ cd ~/c99\n",
    "$ make\n",
    "```\n",
    "    \n",
    "### Fortran\n",
    "\n",
    "```\n",
    "$ cd ~/f90\n",
    "$ make\n",
    "```\n",
    "    \n",
    "This will build an executable named `cg` that you can run with the `./cg` command. You may change the options passed to the compiler by modifying the `CFLAGS` variable in `c99/Makefile` or `FCFLAGS` in `f90/Makefile`. You should not need to modify anything in the Makefile except these compiler flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Identify Parallelism\n",
    "\n",
    "In this step, use the command-line NVPROF profiler, or your preferred performance analysis\n",
    "tool, to idetify the important routines in the application and examine the\n",
    "loops within these routines to determine whether they are candidates for\n",
    "acceleration. Run the command below to gather a CPU profile.\n",
    "\n",
    "```\n",
    "~/c99$ nvprof --cpu-profiling on --cpu-profiling-mode top-down ./cg\n",
    "Rows: 8120601, nnz: 218535025\n",
    "Iteration: 0, Tolerance: 4.0067e+08\n",
    "Iteration: 10, Tolerance: 1.8772e+07\n",
    "Iteration: 20, Tolerance: 6.4359e+05\n",
    "Iteration: 30, Tolerance: 2.3202e+04\n",
    "Iteration: 40, Tolerance: 8.3565e+02\n",
    "Iteration: 50, Tolerance: 3.0039e+01\n",
    "Iteration: 60, Tolerance: 1.0764e+00\n",
    "Iteration: 70, Tolerance: 3.8360e-02\n",
    "Iteration: 80, Tolerance: 1.3515e-03\n",
    "Iteration: 90, Tolerance: 4.6209e-05\n",
    "Total Iterations: 100 Total Time: 39.722421s\n",
    "\n",
    "======== CPU profiling result (top down):\n",
    "99.83% main\n",
    "| 75.08% matvec(matrix const &, vector const &, vector const &)\n",
    "| 18.19% waxpby(double, vector const &, double, vector const &, vector const &)\n",
    "| 4.25% dot(vector const &, vector const &)\n",
    "| 2.29% allocate_3d_poission_matrix(matrix&, int)\n",
    "| 0.02% free_matrix(matrix&)\n",
    "|   0.02% munmap\n",
    "0.15% __c_mset8\n",
    "0.02% dot(vector const &, vector const &)\n",
    "\n",
    "======== Data collected at 100Hz frequency\n",
    "```\n",
    "\n",
    "We see from the above output that the `matvec`, `waxpy`, and `dot` routines take up the majority of the runtime of this application. We will focus our effort on accelerating these functions.\n",
    "\n",
    "***NOTE:*** The `allocate_3d_poission_matrix` routine is an initialization\n",
    "routine that can be safely ignored.\n",
    "\n",
    "Documentation for nvprof can be found [here](http://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Express Parallelism\n",
    "\n",
    "Within each of the routines identified above, express the available parallelism\n",
    "to the compiler using either the `acc kernels` or `acc parallel loop`\n",
    "directive. As an example, here's the OpenACC code to add to the `matvec` routine.\n",
    "\n",
    "```\n",
    "void matvec(const matrix& A, const vector& x, const vector &y) {\n",
    "\n",
    "  unsigned int num_rows=A.num_rows;\n",
    "  unsigned int *restrict row_offsets=A.row_offsets;\n",
    "  unsigned int *restrict cols=A.cols;\n",
    "  double *restrict Acoefs=A.coefs;\n",
    "  double *restrict xcoefs=x.coefs;\n",
    "  double *restrict ycoefs=y.coefs;\n",
    "\n",
    "#pragma acc kernels\n",
    "  {\n",
    "    for(int i=0;i<num_rows;i++) {\n",
    "      double sum=0;\n",
    "      int row_start=row_offsets[i];\n",
    "      int row_end=row_offsets[i+1];\n",
    "      for(int j=row_start;j<row_end;j++) {\n",
    "        unsigned int Acol=cols[j];\n",
    "        double Acoef=Acoefs[j];\n",
    "        double xcoef=xcoefs[Acol];\n",
    "        sum+=Acoef*xcoef;\n",
    "      }\n",
    "      ycoefs[i]=sum;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Add the necessary directives to each routine **one at a time** in order of importance. After adding the directive, recompile the code, check that the answers have remained the same, and note the performance difference from your\n",
    "change.\n",
    "\n",
    "```\n",
    "$ make\n",
    "pgc++ -fast -acc -ta=tesla:managed -Minfo=accel main.cpp -o cg\n",
    "\n",
    "matvec(const matrix &, const vector &, const vector &):\n",
    "      8, include \"matrix_functions.h\"\n",
    "          15, Generating copyout(ycoefs[:num_rows])\n",
    "              Generating\n",
    "copyin(xcoefs[:],Acoefs[:],cols[:],row_offsets[:num_rows+1])\n",
    "          16, Loop is parallelizable\n",
    "              Accelerator kernel generated\n",
    "              Generating Tesla code\n",
    "              16, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n",
    "          20, Loop is parallelizable\n",
    "```\n",
    "\n",
    "The performance may slow down as you're working on this step. Be sure\n",
    "to read the compiler feedback to understand how the compiler parallelizes the\n",
    "code for you. If you are doing the C/C++ lab, it may be necessary to declare\n",
    "some pointers as `restrict` in order for the compiler to parallelize them. You\n",
    "will know if this is necessary if the compiler feedback lists a \"complex loop\n",
    "carried dependency.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Re-Profile Application\n",
    "\n",
    "Once you have added the OpenACC directives to your code, you should obtain a\n",
    "new profile of the application. For this step, use the NVIDIA Visual Profiler\n",
    "to obtain a GPU timeline and see how the the GPU computation and data movement\n",
    "from CUDA Unified Memory interact. \n",
    "\n",
    "- If you are doing this lab via qwikLABs, launch the NVIDIA Visual Profiler by following these steps:\n",
    " 1. First connect to the Ubuntu remote desktop. There are few ways to do this:\n",
    "   * Using the <a href=\"/vnc\" target=\"_blank\">browser-based VNC client</a> (easiest but lowest performance of the options)\n",
    "   * Connecting with a local VNC client to **ec2-54-92-248-21.compute-1.amazonaws.com** using password **FcN7Dy6BtVs**\n",
    "   * Connecting with NoMachine 4.x or 5.x client to **ec2-54-92-248-21.compute-1.amazonaws.com** with username **ubuntu** and password **FcN7Dy6BtVs** on port 4000 - the NX protocol (best performance of the options)\n",
    " 2. Once you're connected to the Ubuntu remote desktop, click the Ubuntu icon in the upper-left of the desktop, and type *nvvp* in the search box and hit enter.\n",
    " 3. After a short-time you will see the NVIDIA Visual Profiler application\n",
    "- If you are doing this lab on your own machine, either launch Visual Profiler\n",
    "  from its application link or via the `nvvp` command.\n",
    "\n",
    "Once Visual Profiler has started, create a new session by selecting *File -> New\n",
    "Session*. Then select the executable that you built by pressing the *Browse*\n",
    "button next to *File*, browse to `/home/ubuntu/c99` or `/home/ubuntu/f90`, \n",
    "select `cg`,  and then press *Next*. On the next screen ensure that\n",
    "*Enable unified memory profiling* is checked and press *Finish*. The result\n",
    "should look like the image below. Experiment with Visual Profiler to see what\n",
    "information you can learn from it.\n",
    "\n",
    "![Image of NVIDIA Visual Profiler after completing lab 2 with the kernels\n",
    "directive](https://github.com/NVIDIA-OpenACC-Course/nvidia-openacc-course-sources/raw/master/labs/lab2/visual_profiler_lab2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After completing the above steps for each of the 3 important routines your application should show a speed-up over the unaccelerated version. You can verify this by removing the `-ta` flag from your compiler options. \n",
    "\n",
    "If you have code like what is in the `solution.kernels` or `solution.parallel` directories, you should see a roughly 14% speed-up over the CPU version.  If you were to use a GPU such as a K40 vs the K520 in this g2.2xlarge instance, you can get speeds closer to 8.4 seconds!  Here's a table showing the speeds on different CPUs and GPUs:\n",
    "\n",
    "| Processor | Time |\n",
    "| --------- | ---- |\n",
    "| Haswell CPU  | 30.519176 | \n",
    "| K40 GPU      | 8.460459 | \n",
    "| g2.2xlarge CPU | 36.647187 |\n",
    "| g2.2xlarge GPU | 32.084089 |\n",
    "\n",
    "In the next lecture and lab we will replace CUDA Unified Memory with explicit memory management using OpenACC and then further optimize the loops using the OpenACC `loop` directive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task\n",
    "\n",
    "1. If you used the `kernels` directive to express the parallelism in the code,\n",
    "try again with the `parallel loop` directive. Remember, you will need to take\n",
    "responsibility of identifying any reductions in the code. If you used \n",
    "`parallel loop`, try using `kernels` instead and observe the differences both in\n",
    "developer effort and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommend you go to your browsers File menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well.\n",
    "\n",
    "You can also execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -f openacc_files.zip\n",
    "zip -r openacc_files.zip ~/c99/* ~/f90/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** executing the above zip command, you should be able to download the zip file [here](files/openacc_files.zip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
